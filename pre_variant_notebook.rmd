---
title: "Pre-variant Pipeline"
output: html_notebook
---

This document will work you through a basic variant calling pipeline with `samtools` and `GATK`, calling out specific locations where filtering choices are made. The following software packages are required, with the versions used here noted:

* `GATK` v4.4.0.0
* `picard` v3.1.1
* `samtools` v1.13
* `bwa` v0.7.17-r1188
* `vcftools` v0.1.16
* `bcftools` v1.13

And, for fetching data,

* `sratoolkit` v2.11.3

As this notebook is written, these are all expected to be available on the system path.

The following R packages are also required:

* `data.table` v1.14.8
* `foreach` v1.5.2
* `parallel` v4.2.3
* `doParallel` v1.0.17

Please note that this notebook is set-up to use a small dataset--while it is written to run moderately in parallel, it may be too slow for use with very large datasets. If access to a computing cluster is available, custom scripts based on these which use run management tools like `slurm` may be preferable.

Please note that some of this code is and scripts are adapted from the R package `alignR` by W Hemstrom, which is not yet fully released. The `bash` scripts we are going to use are:

* `run_align.sh`: This will run the alignments and initial mapping filters.
* `add_RGs.sh`: This will use `picard` to add read-groups to our data. Note that this script is customized slightly for this particular dataset and should be tweaked if used with other data.
* `run_haplotype_caller.sh`: This will run `GATK`'s `HaplotypeCaller` on each sample.
* `run_GenomicsDBImport.sh`: This will run `GATK`'s `GenomicsDBImport` on each sample.
* `run_GenotypGVCFs.sh`: This will run `GATK`'s `GenotypeGVCFs` on each database created by `GenomicsDBImport`.
* `run_VariantFiltration.sh`: We'll use this to filter the final resulting data using `GATK`'s `VariantFiltration` tool.

If you use this notebook or these scripts for publication, please cite this paper for now. These scripts must be located in the working directory, and are available at [here](https://github.com/ChristieLab/filtering_simulation_paper/tree/main/workflow_notebooks).

```{r setup}
library(data.table); library(foreach)
```


# Parameter control

The following R chunk controls the filtering parameters this script will use. To run this entire analysis with a different set of filters, they can simply be changed here! Everything that follows can then be re-run as written with no further changes necissary.

```{r set_filters}
# run parameters
par <- 4 # for all of these jobs, how many processing threads should we use?
mem <- 8 # number of GB of ram to allow for alignment and calling jobs
temp_dir <- tempdir() # temporary directory for alignment and calling jobs
run_chrs <- 1:29 # which chromosomes will we run?
java_path <- "~/bin/jdk-17.0.9/bin/java" # path to a java 17+ install
picard_path <- "~/bin/picard.jar" # path to picard .jar
gatk4_path <- "~/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar" # path to gatk
platform <- "ILLUMINA" # sequencing platform

# alignment filters
samtools_min_map_q <- 5 # Mapping quality cuttoff for samtools. A low pass is useful to speed up further analysis.
remove_PCR_duplicates <- TRUE # Set to TRUE to remove PCR duplicates
remove_improperly_paired_reads <- TRUE # Set to TRUE to remove improperly paired reads

# Post-calling hard filters, see below
QD <- 2
FS <- 60
SOR <- 3
MQ <- 40
MQRankSum <- -12.5
ReadPosRankSum <- -8
min_genotype_quality <- 13
```

# Fetch example data

We are going to use a small example dataset of 5 individuals from two different populations of monarch butterflies located on Guam and Rota Islands, which are part of the Mariana Islands. These are from the paper [here](https://doi.org/10.1111/mec.16592).

Below is the list of accession numbers for the data we will be using and their populations of origin:

```{r define_SRAs}
SRA_info <- data.table(SRA = c("SRR19628125", "SRR19628055", "SRR19628192", "SRR19628238", "SRR19628132", "SRR19628059", "SRR19628194", "SRR19628231", "SRR19628224", "SRR19628056"),
                       pop = rep(c("Rota", "Guam"), each = 5))
SRA_info$new_name <- paste0(SRA_info$pop, "_", rep(1:5, length.out = nrow(SRA_info)))

```

We will use the `sratoolkit` to fetch these. 

```{r fetch_data}
# fetch and rename each file, looping in parallel for efficiency:
cl <- parallel::makePSOCKcluster(par)
doParallel::registerDoParallel(cl)

foreach(q = 1:nrow(SRA_info)) %dopar% {
  # prefetch
  cmd <- paste0("fasterq-dump --split-files ", SRA_info[q,1])
  system(cmd)
  
  # rename
  file.rename(paste0(SRA_info[q,1], "_1.fastq"), paste0(SRA_info[q,3], "_1.fastq"))
  file.rename(paste0(SRA_info[q,1], "_2.fastq"), paste0(SRA_info[q,3], "_2.fastq"))
}

cl <- parallel::makePSOCKcluster(par)
doParallel::registerDoParallel(cl)
```

# Align and run initial filters

In this section, we will align each of our fastq files to the monarch butterfly genome, which we will first download and index with both `bwa`...:

```{bash download_genome}
# Download and index the genome
wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/731/565/GCA_009731565.1_Dplex_v4/GCA_009731565.1_Dplex_v4_genomic.fna.gz

# rename, rezip as bgzip, and index
mv GCA_009731565.1_Dplex_v4_genomic.fna.gz monarch_genome.fna.gz
gunzip monarch_genome.fna.gz
bgzip monarch_genome.fna
bwa index monarch_genome.fna.gz
```

`picard`...:

```{r picard_index}
cmd <- paste0(java_path, " -jar ", picard_path, " CreateSequenceDictionary R= monarch_genome.fna.gz O= monarch_genome.dict")
system(cmd)
```

and `samtools faidx`:

```{r samtools_index}
cmd <- "samtools faidx monarch_genome.fna.gz"
system(cmd)
```


Next, we'll run the script `run_align.sh` for each set of paired reads we downloaded to align and filter our data based on the parameters we set above using `bwa` and `samtools`, then use the `add_RGs.sh` script to add read-group information to each resulting `bam` file with `picard`.

```{r run_align}
cl <- parallel::makePSOCKcluster(par)
doParallel::registerDoParallel(cl)

foreach(q = 1:nrow(SRA_info)) %dopar% {
  
  # run the alignment
  cmd <- paste0("bash run_align.sh ",
                paste0(SRA_info[q,3], "_1"), " ",
                paste0(SRA_info[q,3], "_2"), " ",
                "monarch_genome.fna.gz ",
                samtools_min_map_q, " ",
                as.numeric(remove_PCR_duplicates), " ",
                as.numeric(remove_improperly_paired_reads), " ")
  cat("Running: ", cmd, "\n")
  system(cmd)

  # rename file
  file.rename(paste0(SRA_info[q,3], "_1.sort.flt.bam"),
              paste0(SRA_info[q,3], ".sort.flt.bam"))
  file.rename(paste0(SRA_info[q,3], "_1.sort.flt.bam.bai"),
              paste0(SRA_info[q,3], ".sort.flt.bam.bai"))
  
  # add read groups
  cmd2 <- paste0("bash add_RGs.sh ",
                 paste0(SRA_info[q,3], ".sort.flt"), " ",
                 paste0(SRA_info[q,3], "_1.fastq"), " ",
                 SRA_info[q,3], " ",
                 java_path, " ",
                 picard_path, " ",
                 platform)
  
  system(cmd2)
}

parallel::stopCluster(cl)
```

# Run HaplotypeCaller

Running `GATK` is a multi-step process. First, we need to call haplotypes on each sample, which we can do using the `run_HaplotypeCaller.sh` script. There are no filtering steps here:

```{r run_HaplotypeCaller}
for(q in 1:nrow(SRA_info)) {
  # run Haplotype Caller
  
  ttempdir <- file.path(temp_dir, paste0("HC_q", q))
  dir.create(ttempdir)
  cmd <- paste0("bash run_HaplotypeCaller.sh ", 
                paste0(SRA_info[q,3], ".sort.flt.RG.bam"), " ", 
                "monarch_genome.fna.gz ",
                ttempdir, " ",
                mem, " ",
                java_path, " ",
                gatk4_path, " ")
  
  system(cmd)
  unlink(ttempdir, recursive = TRUE)
}
```

# Run GenomeDBImport

Next, we will make databases from the `gvcf` files we produced for each individual in the previous step. First, we need to build a sample map for our samples and a bed file for each chromosome:

```{r prepare_import}
# prepare sample map and save
sample_map <- data.table(samp = SRA_info$new_name, file = paste0(SRA_info$new_name, ".sort.flt.RG.bam.hapcalls.gvcf.gz"))
fwrite(sample_map, "hapmap.txt", sep = "\t", col.names = F, row.names = F)

# prepare bedfiles
bed <- data.table(chr = c("CM019184.1", "CM019185.1", "CM019186.1", "CM019187.1", "CM019188.1", "CM019189.1", "CM019190.1", "CM019191.1", "CM019192.1", "CM019193.1", "CM019194.1", "CM019195.1", "CM019196.1", "CM019197.1", "CM019198.1", "CM019199.1", "CM019200.1", "CM019201.1", "CM019202.1", "CM019203.1", "CM019204.1", "CM019205.1", "CM019206.1", "CM019207.1", "CM019208.1", "CM019209.1", "CM019210.1", "CM019211.1", "CM019212.1"),
                  start = 1,
                  end = c("10631990", "10450245", "10414450", "10184249", "10039743", "9950675", "9826000", "9622331", "9320052", "9273636", "9209872", "8907986", "8886595", "8615111", "8545482", "8449262", "8166185", "7845940", "7367651", "7243763", "7084546", "6814233", "6514101", "5644642", "5380037", "4150802", "3992836", "3625418", "3411326"))
bed <- bed[run_chrs,]

for(i in 1:nrow(bed)){
  fwrite(bed[i,], paste0(bed$chr[i], ".bed"), col.names = F, row.names = F, quote = F, sep = "\t")
}
```

We'll now run `GenomeDBImport` for each bed file to make our final bed files, using the `run_GenomicsDBImport.sh` script.

```{r run_GenomicsDBImport}
for(i in 1:nrow(bed)){
  cmd3 <- paste0("bash run_GenomicsDBImport.sh ",
                "hapmap.txt ",
                paste0(bed$chr[i], ".bed"), " ",
                mem, " ",
                temp_dir, " ",
                par, " ",
                java_path, " ",
                gatk4_path, " ",
                5) # batch size for processing, number of samples at a time
  
  system(cmd3)
}

```

# Run GenotypeGVCFs
Lastly, we'll run `GenotypeGVCFs` to call genotypes for each db we produced in the previous step.

```{r run_GenotypeGVCFs}
for(i in 1:nrow(bed)){
  cmd4 <- paste0("bash run_GenotypeGVCFs.sh ",
                paste0(bed$chr[i], ".bed"), " ",
                "monarch_genome.fna.gz ",
                mem, " ",
                temp_dir, " ",
                java_path, " ",
                gatk4_path)
  
  system(cmd4)
}
```

# Run hard-filters

Now we will run a set of filters on our variant calls to produce our final set of genotypes. Specifically, we're going to first apply `GATK`'s [hard filters](https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants), select only bi-allelic SNPs, and then mask out individual genotypes where the sequencing confidence is low given the coverage we have. We use the `run_VariantFiltration.sh` script to do this, which calls `GATK`'s `VariantFiltration` tool, `vcftools`, and `bcftools`.

This is where we use most of the filtering parameters that we set at the start of this document.

```{r hard_filters}

cl <- parallel::makePSOCKcluster(par)
doParallel::registerDoParallel(cl)


foreach(q = 1:nrow(bed)) %dopar% {
  cmd5 <- paste0("bash run_VariantFiltration.sh ", 
                paste0("raw_", bed[q,1], ".bed"), " ", 
                "monarch_genome.fna.gz ",
                format(QD, nsmall = 1), " ",
                format(FS, nsmall = 1), " ",
                format(SOR, nsmall = 1), " ",
                format(MQ, nsmall = 1), " ",
                format(MQRankSum, nsmall = 1), " ",
                format(ReadPosRankSum, nsmall = 1), " ",
                min_genotype_quality, " ",
                mem, " ",
                java_path, " ",
                gatk4_path)
  
  system(cmd5)
}

cl <- parallel::makePSOCKcluster(par)
doParallel::registerDoParallel(cl)

```

Our final VCF files for each chromosome will then be stored in files named `hard_filt_pass_raw_[CHRNAME].bed.recode.vcf`. 

# Merging across chromosomes to create the final VCF file
Lastly, we will merge across chromosomes to create a complete VCF file. We'll use `bcftools` for this

```{bash concat}
bcftools concat hard_filt_pass_raw_*.bed.recode.vcf > final_monarch_variants.vcf
```

The final, concatenated genotypes will then be located in `final_monarch_variants.vcf`.

We will use this data in the second notebook, `post_variant_notebook.rmd`.
