---
title: "RDA_analyses.Rmd"
author: "Hugo DENIS"
date: "2025-06-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Script to perform RDA on host and symbiont population structure from A.spathulata across the Coral Sea

#Load packages
```{r}
library(vegan)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(parallel)
library(adespatial)
library(geosphere)
```


###1. Perform db-RDA on symbiont populations structure 

#Load the symbiont kmer data
```{r}
path="C:/Users/Hugo/Documents/Data/RDA_analyses/"
path_save="C:/Users/Hugo/Documents/Figures/Symbionts/dbRDA/"

#Load kmer D2S matrix
kmer_mat=as.matrix(read.table(paste0(path,"D2s_50M_dist_matrix_aspat_clean.txt"),check.names = F))

#Load number of symbiont reads recovered in each sample
#We filter out samples that had initially less bp than 50M (=340000*150, the threshold at which we downsampled all samples)
Nreads=read.table(paste0(path,"Mapped_symbiont_read_counts_fasta.txt"),col.names=c("sample","Nreads")) %>% mutate(wgsID_short=sub("_.*","",sub(".*Aspat-","",basename(sample))))
kmer_mat=kmer_mat[rownames(kmer_mat) %in% Nreads$wgsID_short[Nreads$Nreads>340000],colnames(kmer_mat) %in% Nreads$wgsID_short[Nreads$Nreads>340000]]

#Exclude one sample that is missing spatial coordinates
kmer_mat=kmer_mat[rownames(kmer_mat)!="CBLM-1655",colnames(kmer_mat)!="CBLM-1655"]
```

#Load environmental data and principal components from host PCA 
```{r}
#Environmental variables
env=read.csv(paste0(path,"all_env_metrics_aspat_sites.csv"))

#Host principal components
host_pc=read.table(paste0(path,"Host_PCs_aspat_clean_all_chr_SNP_filtered_2_20mis_noclones_recoded_MAF0.05_LD0.2.txt"),header=T)
rownames(host_pc)=host_pc$wgsID_short
host_pc=host_pc[,paste0("PC",as.character(seq(1,10)))]
```

#Load metadata
```{r}
meta_colony=read.table(paste0(path,"Colony_metadata_Aspat_GBR_NC.txt"),header=T,sep="\t")
```


#Compute dbMem eigenvectors for the subset of colonies used in the analysis
#Following the method of Borcard & Legendre (2002, 2004)
```{r}
rownames(meta_colony)=meta_colony$wgsID_short
coord=meta_colony[rownames(kmer_mat),c("wgsID_short","Lon","Lat")]
coord$Lon=as.numeric(coord$Lon)
coord$Lat=as.numeric(coord$Lat)

#Discard individuals that have missing Lon Lat (1 colony)
coord=coord %>% subset(!is.na(Lon) & !is.na(Lat))

#At NC and CB sites colonies spatial coordinates were not collected and only the sites coordinates were used
#As this will create an issue in the dbMem computation, we will jitter points around the site mean (by a distance of maximum 300m which si the size of sites that were explored)
distance_limit=300
jittered_coord=coord
for(i in 1:nrow(coord)){
  if(coord$wgsID_short[i] %in% meta_colony$wgsID_short[meta_colony$Country=="New_Caledonia"]){
    lat_mean <- coord[i, "Lat"]
    lon_mean <- coord[i, "Lon"]
    
    #Random distance and angle in radians
    distance <- runif(1, min = 0, max = distance_limit)
    angle <- runif(1, min = 0, max = 2 * pi)
    
    # Jittered coordinates (Lat, Lon) using polar coordinates (distance and angle)
    lat_jitter <- lat_mean + (distance / 111320) * cos(angle) # Approximate conversion to degrees
    lon_jitter <- lon_mean + (distance / (111320 * cos(lat_mean * pi / 180))) * sin(angle) # Adjusted for latitude
    
    jittered_coord[i, "Lat"] <- lat_jitter
    jittered_coord[i, "Lon"] <- lon_jitter
    
  }
}

#Save jittered coordinates
write.table(jittered_coord,paste0(path,"Jiitered_coordinates_symbiont_subset.txt"),row.names=F)

#Compute spatial distance using haversine distance (more accurate than euclidean distance for spatial coordinates)
n <- nrow(jittered_coord)
hav_matrix <- matrix(0, nrow = n, ncol = n)
rownames(hav_matrix) <- rownames(jittered_coord)
colnames(hav_matrix) <- rownames(jittered_coord)

# Fill the matrix with Haversine distances
for(i in 1:n){
  for(j in 1:n){
    if(i != j){
      d <- distHaversine(jittered_coord[i, c("Lon", "Lat")], jittered_coord[j, c("Lon", "Lat")])
      hav_matrix[i, j] <- d / 1000  # convert meters to kilometers
    }
  }
}

# Convert to distance matrix object
hav_dist <- as.dist(hav_matrix)
#Save distance object 
saveRDS(hav_dist,paste0(path,"Haversine_distance_matrix_symbiont_subset.rds"))

#Run dbmem
dbmem_vars <- adespatial::dbmem(hav_dist)
dbmem_vars$wgsID_short=rownames(dbmem_vars)

#Save dbMems values ( 4 eigenvectors are found)
saveRDS(dbmem_vars,paste0(path,"dbMems_symbiont_subset.rds"))

dbmem_vars=readRDS(paste0(path,"dbMems_symbiont_subset.rds"))
dbmem_vars$wgsID_short=rownames(dbmem_vars)

# Eigenvalues
eigenvalues <- attributes(dbmem_vars)$values

# Proportion of variance explained
var_explained <- eigenvalues / sum(eigenvalues)

# Cumulative variance explained
cumulative_var_explained <- cumsum(var_explained)

plot(seq(1,14),cumulative_var_explained)

# Print eigenvalues and cumulative variance explained
print(data.frame(eigenvalues, var_explained, cumulative_var_explained))

#4 dbMems eigenvectors explain 90% of the variance 
```

#create a predictor table with all predictor variables in the same order as in the response matrix 
```{r}
pred=meta_colony %>% dplyr::select(wgsID_short,locationID) %>% merge(env,by.x="locationID",by.y="Site.name")
pred=pred  %>% merge(host_pc,by="wgsID_short")
pred=pred %>% merge(dbmem_vars,by="wgsID_short")
pred=pred %>% merge(Nreads,by="wgsID_short")

#Make sure only individuals in the response matrix are kept
kmer_mat=kmer_mat[rownames(kmer_mat) %in% pred$wgsID_short,colnames(kmer_mat) %in% pred$wgsID_short]
kmer_dist=as.dist(kmer_mat)
rownames(pred)=pred$wgsID_short
pred=pred[rownames(kmer_mat),]

#Confirm predictors and response variables or ordered the same
sum(rownames(pred)!=rownames(kmer_mat))

#Standardize predictors 
pred_stand=decostand(pred %>% dplyr::select(-c(wgsID_short,locationID,Country,sample)),method="standardize")
```

#Plot correlation between environmental predictors 
```{r}
# #Plot correlation between environmental predictors 
# pred_stand_sub=pred_stand[,c("Km_to_coastline","Lat","DHW_avg_5km","Kd490_median","CF_median","DHW_collection","SST_AR","SSTmean_change","Chla_median","MMM_5km","DHW_freq_sup4_5km","DHW_max_5km","TSA_DHW_mean_5km","SST_average")]
# cor=cor(pred_stand_sub,method="pearson")
# 
# library(corrplot)
# png(height=465, width=465, file=paste0(figpath,"Env/Env_predictors_pearson_correlation.png"),res=300,units='mm')
# corrplot::corrplot(cor, order = 'hclust', addrect = 6,col = COL2('RdYlBu', 10),cl.pos = 'b',tl.cex=2,cl.cex = 2,tl.col='black')
# dev.off()

```


#Perform partial db-RDA, to separate the effect of environmental variables, geography and host population structure, several dbRDA are built
#First we build a model using only environmental variables and we perform forward selection to retain only significant predictors 

#Model 1 : Env + dbMems + Host_structure (PC1,PC2,PC3) | (Nreads)
#Model 2 :  Env | (Nreads + Host_structure + dbMems)
#Model 3 : Host_structure | (Nreads +Env + dbMems)
#Model 4 : dbMems | (Nreads +Env + Host structure)

```{r}
#Perform PCoA
pcoa <- cmdscale(kmer_dist, eig = TRUE, k = 10,add=TRUE)
pcoa_scores <- data.frame(pcoa$points)

#Check for negative eigenvalues
pcoa$eig[pcoa$eig<0]
#There are not negative eigenvalues therefore we proceed with a standard dbRDA without negative eigenvalue correction 

#Perform partial dbRDA conditioned by Nreads
dbRDA = rda(
  pcoa_scores[,1:10] ~ Km_to_coastline + Lat + DHW_avg_5km +
    Kd490_median + CF_median + SST_AR_1km + SSTmean_change +
    Chla_median + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km +
    TSA_DHW_mean_5km + DHW_freq_sup8 + Condition(Nreads),
  data = pred_stand
)

#Look at variant inflation factor and drop variables progressively, starting with the one with the highest VIF until all are below 5
vif.cca(dbRDA)

dbRDA = rda(
  pcoa_scores[,1:10] ~ Km_to_coastline +  
    Kd490_median + CF_median + SST_AR_1km + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km + DHW_freq_sup8_1km+ Condition(Nreads),
  data = pred_stand
)

#Look at variant inflation factor and drop variables progressively, starting with the one with the highest VIF until all are below 10
vif.cca(dbRDA)

#Variables sucessively removed
#TSA_DHW_mean_5km
#DHW_avg_5km
#SST average
#Chla median
#MMM_5km
#Km to coastline 
#Lat

#get summary of the model -> Only <10% of the variance is explained by predictors 
sum=summary(dbRDA)
out=capture.output(summary(dbRDA))
saveRDS(sum,paste0(path,"dbRDA/pdbRDA_model_summary_envonly_340k.rds"))
cat(out,file=paste0(path,"dbRDA/pdbRDA_model_summary_envonly_340k.txt"),sep="\n")

#Look at model significance : the model is significant
signif=anova(dbRDA)
saveRDS(signif,paste0(path,"dbRDA/pdbRDA_model_significance_envonly_340k.rds"))
write.table(data.frame(signif),paste0(path,"dbRDA/pdbRDA_anova_envonly_340k.txt"))

#get model adjusted R.squared : 37%
RsquareAdj(dbRDA)

#Compare constrained variance to variance explained by best predictors of unconstrained principal component analysis
dbPCA=rda(pcoa_scores[,1:10]~1)
#Extract variance explained by nth PCA axis (equal to number of parameters in model)
PCA1_8 <- scores (dbPCA, display = 'sites', choices = 1:8)
dbRDA_PCA12 <- rda (pcoa_scores[,1:10] ~ PCA1_8)
RsquareAdj(dbRDA_PCA12)$r.squared
#The fist 7 axis explain 17% of the variance thus our variables explain 0.37/0.94 = 39% so the model is accounting for a significant part of the variation

Rsquared=data.frame(Model=c("dbRDA","Unconstrained PCA"),Rsquared=c(RsquareAdj(dbRDA)$r.squared,RsquareAdj(dbRDA_PCA12)$r.squared),Rsquared.adj=c(RsquareAdj(dbRDA)$adj.r.squared,RsquareAdj(dbRDA_PCA12)$adj.r.squared))
write.table(Rsquared,paste0(path,"dbRDA/pdbRDA_RsquaredAdj_envonly_340k.txt"),row.names=F)

#Look at which dbRDA axis is significant
sum_axis=anova(dbRDA, by="axis",  permutations =500)
saveRDS(sum_axis,paste0(path,"dbRDA/pdbRDA_axes_significance_envonly_500k.rds"))
#The first 3 axis are significant 

#Stepwise selection of predictors 
dbRDA.sel <- ordiR2step(rda(pcoa_scores[,1:10]~1,data=pred_stand), # lower model limit (simple!)
               scope = formula(dbRDA), # upper model limit (the "full" model)
               direction = "forward",
               R2scope = TRUE, # can't surpass the "full" model's R2
               pstep = 1000,
               trace = FALSE) # change to TRUE to see the selection process!
saveRDS(dbRDA.sel,paste0(path,"dbRDA/pdbRDA_variable_selection_envonly_500k.rds"))

write.table(data.frame(dbRDA.sel$anova),paste0(path,"dbRDA/pdbRDA_selectVariables_envonly_500k.txt"))

#Final model 
dbRDA=dbRDA = rda(pcoa_scores[,1:10] ~ SST_AR_1km + MMM_5km + DHW_freq_sup4_5km,
  data = pred_stand,add=TRUE
)

summary(dbRDA)

RsquareAdj(dbRDA)
```

#Now that we have our set of significant environmental predictors : SST_AR, DHW_freq_sup4_5km, MMM_5Km we will test the partioning of variance across environment, geograhy and host structure

#Model 1 : Env + dbMems (dbMEM1,2,3,4) + Host_structure (PC1,PC2,PC3) | (Nreads)
#Model 2 :  Env | (Nreads + Host_structure + dbMems)
#Model 3 : Host_structure | (Nreads +Env + dbMems)
#Model 4 : dbMems | (Nreads +Env + Host structure)

```{r}
Model1=rda(
  pcoa_scores[,1:10] ~ SST_AR_1km + MMM_5km + DHW_freq_sup4_5km + PC1 + PC2 + PC3 + MEM1 + MEM2 + MEM3 + MEM4 + Condition(Nreads),
  data = pred_stand
)
RsquareAdj(Model1)
#capture.output(summary(Model1))

Model2=rda(
  pcoa_scores[,1:10] ~ SST_AR_1km + MMM_5km + DHW_freq_sup4_5km + Condition(PC1) + Condition(PC2) + Condition(PC3) + Condition(MEM1) + Condition(MEM2) + Condition(MEM3) + Condition(MEM4) + Condition(Nreads),
  data = pred_stand
)
RsquareAdj(Model2)
#capture.output(summary(Model2))

Model3=rda(
  pcoa_scores[,1:10] ~ PC1 + PC2 + PC3 + Condition(SST_AR_5km) + Condition(MMM_5km) + Condition(DHW_freq_sup4_5km)  + Condition(MEM1) + Condition(MEM2) + Condition(MEM3) + Condition(MEM4) + Condition(Nreads),
  data = pred_stand
)
RsquareAdj(Model3)
#capture.output(summary(Model3))

Model4=rda(
  pcoa_scores[,1:10] ~ MEM1 + MEM2 + MEM3 + MEM4 + Condition(PC1) + Condition(PC2) + Condition(PC3) + Condition(SST_AR_1km) + Condition(MMM_5km) + Condition(DHW_freq_sup4_5km) + Condition(Nreads),
  data = pred_stand
)
RsquareAdj(Model4)
#capture.output(summary(Model4))

```

###2. Perform RDA on host population structure

#Compute dbMem eigenvectors for the subset of colonies used in the analysis
#Following the method of Borcard & Legendre (2002, 2004)
```{r}
rownames(meta_colony)=meta_colony$wgsID_short
coord=meta_colony[rownames(host_pc),c("wgsID_short","Lon","Lat")]
coord$Lon=as.numeric(coord$Lon)
coord$Lat=as.numeric(coord$Lat)

#Discard individuals that have missing Lon Lat (1 colony)
coord=coord %>% subset(!is.na(Lon) & !is.na(Lat))

#At NC and CB sites colonies spatial coordinates were not collected and only the sites coordinates were used
#As this will create an issue in the dbMem computation, we will jitter points around the site mean (by a distance of maximum 300m which si the size of sites that were explored)
distance_limit=300
jittered_coord=coord
for(i in 1:nrow(coord)){
  if(coord$wgsID_short[i] %in% meta_colony$wgsID_short[meta_colony$Country=="New_Caledonia"]){
    lat_mean <- coord[i, "Lat"]
    lon_mean <- coord[i, "Lon"]
    
    #Random distance and angle in radians
    distance <- runif(1, min = 0, max = distance_limit)
    angle <- runif(1, min = 0, max = 2 * pi)
    
    # Jittered coordinates (Lat, Lon) using polar coordinates (distance and angle)
    lat_jitter <- lat_mean + (distance / 111320) * cos(angle) # Approximate conversion to degrees
    lon_jitter <- lon_mean + (distance / (111320 * cos(lat_mean * pi / 180))) * sin(angle) # Adjusted for latitude
    
    jittered_coord[i, "Lat"] <- lat_jitter
    jittered_coord[i, "Lon"] <- lon_jitter
    
  }
}

#Save jittered coordinates
write.table(jittered_coord,paste0(path,"Jiitered_coordinates_host_subset.txt"),row.names=F)

#Compute spatial distance using haversine distance (more accurate than euclidean distance for spatial coordinates)
n <- nrow(jittered_coord)
hav_matrix <- matrix(0, nrow = n, ncol = n)
rownames(hav_matrix) <- rownames(jittered_coord)
colnames(hav_matrix) <- rownames(jittered_coord)

# Fill the matrix with Haversine distances
for(i in 1:n){
  for(j in 1:n){
    if(i != j){
      d <- distHaversine(jittered_coord[i, c("Lon", "Lat")], jittered_coord[j, c("Lon", "Lat")])
      hav_matrix[i, j] <- d / 1000  # convert meters to kilometers
    }
  }
}

# Convert to distance matrix object
hav_dist <- as.dist(hav_matrix)

#Save distance object 
saveRDS(hav_dist,paste0(path,"Haversine_distance_matrix_host_subset.rds"))

#Run dbmem
dbmem_vars <- adespatial::dbmem(hav_dist)
dbmem_vars$wgsID_short=rownames(dbmem_vars)

#Save dbMems values ( 4 eigenvectors are found)
saveRDS(dbmem_vars,paste0(path,"dbMems_host_subset.rds"))

dbmem_vars=readRDS(paste0(path,"dbMems_host_subset.rds"))

# Eigenvalues
eigenvalues <- attributes(dbmem_vars)$values

# Proportion of variance explained
var_explained <- eigenvalues / sum(eigenvalues)

# Cumulative variance explained
cumulative_var_explained <- cumsum(var_explained)

plot(seq(1,10),cumulative_var_explained)

# Print eigenvalues and cumulative variance explained
print(data.frame(eigenvalues, var_explained, cumulative_var_explained))

#4 dbMems eigenvectors explain 90% of the variance 
```

#create a predictor table with all predictor variables in the same order as in the response matrix 
```{r}
pred=meta_colony %>% dplyr::select(wgsID_short,locationID) %>% merge(env,by.x="locationID",by.y="Site.name")
pred=pred %>% merge(dbmem_vars,by="wgsID_short")

#Make sure only individuals in the response matrix are kept
host_pc=host_pc[rownames(host_pc) %in% pred$wgsID_short,]

rownames(pred)=pred$wgsID_short
pred=pred[rownames(host_pc),]

#Standardize predictors 
pred_stand=decostand(pred %>% dplyr::select(-c(wgsID_short,locationID,Country)),method="standardize")
```

#Plot correlation between environmental predictors 
```{r}
# #Plot correlation between environmental predictors 
# pred_stand_sub=pred_stand[,c("Km_to_coastline","Lat","DHW_avg_5km","Kd490_median","CF_median","DHW_collection","SST_AR","SSTmean_change","Chla_median","MMM_5km","DHW_freq_sup4_5km","DHW_max_5km","TSA_DHW_mean_5km","SST_average")]
# cor=cor(pred_stand_sub,method="pearson")
# 
# library(corrplot)
# png(height=465, width=465, file=paste0(figpath,"Env/Env_predictors_pearson_correlation.png"),res=300,units='mm')
# corrplot::corrplot(cor, order = 'hclust', addrect = 6,col = COL2('RdYlBu', 10),cl.pos = 'b',tl.cex=2,cl.cex = 2,tl.col='black')
# dev.off()

```


#Perform partial RDA, to separate the effect of environmental variables, geography and host population structure, several dbRDA are built
#First we build a model using only environmental variables and we perform forward selection to retain only significant predictors 

#Model 1 : Env + dbMems + Host_structure (PC1,PC2,PC3) | (Nreads)
#Model 2 :  Env | (Nreads + Host_structure + dbMems)
#Model 3 : Host_structure | (Nreads +Env + dbMems)
#Model 4 : dbMems | (Nreads +Env + Host structure)

```{r}
#RDA conditioned by Nreads
RDA = rda(
  host_pc ~ Km_to_coastline +  DHW_avg_5km +
    Kd490_median + CF_median + SST_AR_1km + SSTmean_change +
    Chla_median + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km +
    TSA_DHW_mean_5km + DHW_freq_sup8 ,
  data = pred_stand
)

#Look at variant inflation factor and drop variables progressively, starting with the one with the highest VIF until all are below 5
vif.cca(RDA)

#Variables sucessively removed
#TSA_DHW_mean_5km
#DHW_avg_5km
#SST average
#Chla median
#SSTmean_change
#Km to coastline 
#Lat


RDA = rda(
  host_pc ~ Km_to_coastline  +
    Kd490_median + CF_median + SST_AR_1km  + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km +
     DHW_freq_sup8_1km ,
  data = pred_stand
)

#Look at variant inflation factor and drop variables progressively, starting with the one with the highest VIF until all are below 10
vif.cca(RDA)
RsquareAdj(RDA)

#get summary of the model -> Only <10% of the variance is explained by predictors 
sum=summary(RDA)
out=capture.output(summary(RDA))
saveRDS(sum,paste0(path,"RDA_model_summary_envonly.rds"))
cat(out,file=paste0(path,"RDA_model_summary_envonly.txt"),sep="\n")

#Look at model significance : the model is significant
signif=anova(RDA)
saveRDS(signif,paste0(path,"RDA_model_significance_envonly.rds"))
write.table(data.frame(signif),paste0(path,"RDA_anova_envonly.txt"))

#get model adjusted R.squared : 63%
RsquareAdj(RDA)

#Look at which dbRDA axis is significant
sum_axis=anova(RDA, by="axis",  permutations =500)
saveRDS(sum_axis,paste0(path,"RDA_axes_significance_envonly.rds"))
#The first 3 axis are significant 

#Stepwise selection of predictors 
RDA.sel <- ordiR2step(rda(host_pc~1,data=pred_stand), # lower model limit (simple!)
               scope = formula(RDA), # upper model limit (the "full" model)
               direction = "forward",
               R2scope = TRUE, # can't surpass the "full" model's R2
               pstep = 1000,
               trace = FALSE) # change to TRUE to see the selection process!
saveRDS(RDA.sel,paste0(path,"RDA_variable_selection_envonly.rds"))

RDA.sel=readRDS(paste0(path,"RDA_variable_selection_envonly_500k.rds"))

write.table(data.frame(RDA.sel$anova),paste0(path,"RDA_selectVariables_envonly.txt"),quote=F)

#Final model = previous model 
RDA=rda(
  host_pc ~ SST_AR_1km + MMM_5km + DHW_freq_sup4_5km,
  data = pred_stand
)


RsquareAdj(RDA)


```

#Now that we have our set of significant environmental predictors : SST_AR, DHW_freq_sup4_5km, MMM_5Km we will test the partioning of variance across environment and geography

#Model 1 : Env + dbMems (dbMEM1,2,3,4) 
#Model 2 :  Env | ( dbMems)
#Model 3 : dbMems | Env

```{r}
Model1=rda(
  host_pc ~ Km_to_coastline  +
    Kd490_median + CF_median + SST_AR_1km  + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km +
     DHW_freq_sup8_1km + MEM1 + MEM2 + MEM3 + MEM4,
  data = pred_stand
)
RsquareAdj(Model1)
#capture.output(summary(Model1))

Model2=rda(
  host_pc ~ Km_to_coastline  +
    Kd490_median + CF_median + SST_AR_1km  + MMM_5km + DHW_freq_sup4_5km + DHW_max_5km +
     DHW_freq_sup8_1km + Condition(MEM1) + Condition(MEM2) + Condition(MEM3) + Condition(MEM4),
  data = pred_stand
)
RsquareAdj(Model2)
#capture.output(summary(Model2))


Model3=rda(
  host_pc ~ MEM1 + MEM2 + MEM3 + MEM4 + Condition(Km_to_coastline)  +
    Condition(Kd490_median) + Condition(CF_median) + Condition(SST_AR_1km)  + Condition(MMM_5km) + Condition(DHW_freq_sup4_5km) + Condition(DHW_max_5km) +
     Condition(DHW_freq_sup8_1km),
  data = pred_stand
)
RsquareAdj(Model3)
#capture.output(summary(Model3))
```